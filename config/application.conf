global {
  spark-options = ${environment.spark-options}
  
  sparkOptions {
    "spark.default.parallelism" = 2 // should normally be set per action with property parallelism (to be implemented)
    "spark.sql.shuffle.partitions" = 2
    "spark.task.maxFailures" = 1
    // TODO: Monitor logs for this Exception to not loose deliveries: java.lang.IllegalArgumentException: CSV header does not conform to the schema.
    "spark.sql.files.ignoreCorruptFiles" = true // Spark job fails if header is different and spark.sql.files.ignoreCorruptFiles is not set
    //"spark.sql.adaptive.enabled" = false // adaptive query execution from Spark 3.0
    "spark.sql.sources.partitionColumnTypeInference.enabled" = false // partition columns should always have type string
    "spark.sql.broadcastTimeout" = 400
  }
  secretProviders = ${environment.secretProviders}
  synchronousStreamingTriggerIntervalSec = 10

  stateListeners = [
      { className = "io.smartdatalake.util.azure.StateChangeLogger"
        options = { workspaceID : ${environment.state_logging.workspaceID},
                    logAnalyticsKey :  ${environment.state_logging.logAnalyticsKey},
                    logType : ${environment.state_logging.logType} } }
    ]
}

shared {
  fileSystemPrefixSDL = ${environment.fileSystemSDL.fileSystemPrefix}
  fileSystemPrefixInput = ${environment.fileSystemInput.fileSystemPrefix}
  hive-db = ${environment.fileSystemSDL.hive-db}
  //hive-dbinput = ${environment.fileSystemInput.hive-db}

}

connections {
  #Input der GVB Daten
  connection-input-gvb {
    type = HadoopFileConnection
    pathPrefix = ${shared.fileSystemPrefixInput}"/gvb-metadata/"
  }


  #Input der codx-winfap Daten
  connection-input-codx-winfap {
    type = HadoopFileConnection
    pathPrefix = ${shared.fileSystemPrefixInput}"/codx-winfap/"
  }

  #Input der derpunkt-collact Daten
  connection-input-derpunkt-collact {
    type = HadoopFileConnection
    pathPrefix = ${shared.fileSystemPrefixInput}"/"
  }

  #Input der Spektra-Vertec Daten
  connection-input-spektra-vertec {
    type = HadoopFileConnection
    pathPrefix = ${shared.fileSystemPrefixInput}"/spektra-vertec/"
  }

  connection-stage {
    type = HadoopFileConnection
    pathPrefix = ${shared.fileSystemPrefixSDL}"/stage/"
  }

  connection-int {
    type = DeltaLakeTableConnection
    db = ${shared.hive-db}
    pathPrefix = ${shared.fileSystemPrefixSDL}"/int/"
  }

  connection-btl {
    type = DeltaLakeTableConnection
    db = ${shared.hive-db}
    pathPrefix = ${shared.fileSystemPrefixSDL}"/btl/"
  }


}

default{

//  inputGVB = {
//    type = ExcelFileDataObject
//    connectionId = connection-input-gvb
//    path =  "~{id}"
//  }
  inputGVB = {
    type = CsvFileDataObject
    connectionId = connection-input-gvb
    path =  "~{id}"
    csv-options {
      delimiter = ";"
      //encoding = UTF-8
      header = true
      //quote = "."
    }
  }
  inputwinfap = {
    type = CsvFileDataObject
    connectionId = connection-input-codx-winfap
    path =  "~{id}".csv
    csv-options {
      delimiter = ";"
      //encoding = UTF-8
      multiline = true
      header = true
      quote = "\""
      escape = "\""
    }
  }

  //
  inputcollact = {
    type = XmlFileDataObject
    connectionId = connection-input-derpunkt-collact
    path =  "~{id}"
    //partitions = [runId]
  }

  //TODO: Neues Schema generien aktivieren --> Damit neue Spalten hinzugefügt werden können
  inputspektra = {
    type = CsvFileDataObject
    connectionId = connection-input-spektra-vertec
    path =  "~{id}".csv
    csv-options {
      delimiter = ";"
      encoding = windows-1252
    }
  }


  Stage = {
    type = CsvFileDataObject
    connectionId = connection-stage
    path =  "~{id}".csv
  }
  StageTmp = {
    type = ParquetFileDataObject
    connectionId = connection-stage
    path =  "~{id}"
  }

  StageXML = {
    type = XmlFileDataObject
    connectionId = connection-stage
    path =  "~{id}"
    schema = "run_id STRING,`Fels01` STRING,`Fels02` STRING,`Fels03` STRING,`Fels04` STRING,`Fels04a` STRING,`Fels05` STRING,`Fels06` STRING,`Fels06a` STRING,`Fels07` BIGINT,`Fels08` STRING,`Fels10` STRING,`Fels11` STRING,`Fels12` STRING,`Fels13` STRING,`Fels14` BIGINT,`Fels15` BIGINT,`Fels16` STRING,`Fels17` STRING,`_xmlns:xsd` STRING,`_xmlns:xsi` STRING,`al_name` STRING,`al_text` STRUCT<`de`: STRING, `fr`: STRING>,`alarm_id` STRING,`conf_name` STRING,`conference_id` BIGINT,`fall` STRING,`koord_link` STRING,`koordinate` STRING,`org_id` BIGINT,`org_name` STRING,`sms_text` STRUCT<`de`: STRING, `fr`: STRING>,`started` TIMESTAMP,`stufe` BIGINT"
    //filenameColumn = "Spaltenname"
    partitions = [run_id]
  }
  Int = {
    type = DeltaLakeTableDataObject
    connectionId = connection-int
    table {
      name = "~{id}"
    }
  }
  Intpartitions = {
    type = DeltaLakeTableDataObject
    connectionId = connection-int
    partitions =  [run_id]
    table {
      name = "~{id}"
    }
  }
  Btl = {
    type = DeltaLakeTableDataObject
    connectionId = connection-btl
    table {
      name = "~{id}"
    }
  }
  Btlpartitions = {
    type = DeltaLakeTableDataObject
    connectionId = connection-btl
    partitions =  [run_id]
    table {
      name = "~{id}"
    }
  }
}
